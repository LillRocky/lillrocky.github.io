<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python爬虫之99藏书网（下）]]></title>
    <url>%2F2018%2F05%2F31%2FPython%E7%88%AC%E8%99%AB%E4%B9%8B99%E8%97%8F%E4%B9%A6%E7%BD%91%EF%BC%88%E4%B8%8B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前情提示上篇文章Python爬虫之99藏书网（上）中，我们获取了99藏书网中所有的图书信息并存储在了一个名为book_list.json的文件中。现在我们继续我们的工作！ 目标确定首先，我们要确定我们的任务目标。我的想法是程序开始运行后，输入我们要爬取的图书名称，如果图书存在，则开始爬取小说，并最终生成txt文档；如果不存在，则给出提示并退出程序。 过程分析由于上篇文章已经分析了具体的内容，所以不再赘述。 开始实施 获取章节目录 12345678910111213141516171819202122# 获取章节列表def get_chapter_list(self, book_link): # url 前缀 baseurl = 'http://www.99lib.net' # 代理 proxie = &#123; 'http': 'http://127.0.0.1:1080', &#125; # 提示正在获取章节列表 print('获取章节列表中...') # 发起请求 res = requests.get(book_link,proxies=proxie) # 获取响应内容 html = BeautifulSoup(res.text, 'html.parser') # 筛选出id为dir的元素中所有的a链接 a_list = html.select('#dir')[0].find_all('a') # 获取所有的a链接的内容与前缀组成完整的url地址列表 link_list = list(map(lambda x: baseurl + x['href'],a_list)) # 提示该图书总的章节数 print('本书共有' + str(len(link_list)) + '章') # 将最终获取的章节链接列表返回 return link_list 获取章节内容为了方便调试，我们先随便找个章节测试一下，地址是http://www.99lib.net/book/8953/318287.htm打开页面后内容如图测试代码如下： 1234567891011121314# 引入相关模块import requestsfrom bs4 import BeautifulSoup# 测试地址url = 'http://www.99lib.net/book/8953/318287.htm'# 发起请求获取响应内容res = requests.get(url)html = BeautifulSoup(res.text, 'html.parser')# 获取段落内容列表contents = html.select('#content')[0].strings# 将列表逐行打印for content in contents: print(content) 结果如下：这里我们会发现一个问题，获取到的章节内容的段落是乱序的，但是页面上显示的是没问题的。我们来找找原因。 问题分析我们来分析内容乱序的原因，在浏览器中打开章节页面，ctrl + u打开网页源代码，我们看到的结果是这样的我们再F12打开chrom开发者工具，看到的内容是这样的和这样的可以看出，开发者工具中看到的内容仍然是乱序的，而且其中所有的段落都添加了一个随机生成的类名，而且通过display属性进行了显示和隐藏，最终中页面上显示出正确排序的段落。这是个很棘手的问题。我们需要知道这个结果是怎样生成的？既然不是在源代码中，那么应该是有js动态加载的，那么是用ajax动态获取的吗？验证一下，在开发者工具中切换到network选项，刷新页面，再点击xhr，我们发现有一个ajax请求，点击，再切换到response选项，我们发现该请求没有任何响应。如下图：这是个dead end！既然不是ajax，那么要不是后台，要么就是js动态生成。如果是后台，我们看不到后台代码，dead！js的话，共有以下，个人水平有限，还原不了，dead！见下图。那么我们来换个思路，不论怎么生成的页面，我们知道，最终用户看到的页面上显示的是完整且正常的内容。有没有办法让我们直接获取页面上显示的内容呢？答案当然是肯定的！！！ 以下内容摘至之后专栏从零开始写Python爬虫 Selenium的介绍：为什么我们要使用这个包呢？在写Python爬虫的时候，最麻烦的不是那些海量的静态网站，而是那些通过JavaScript获取数据的站点。Python本身对js的支持就不好，所以就有良心的开发者来做贡献了，这就是Selenium，他本身可以模拟真实的浏览器，浏览器所具有的功能他一个都不拉下，加载js更是小菜了。 浏览器的选择：在写爬虫的时候，用到最多的就是Selenium的Webdriver， 当然，webdriver也不可能支持所有的浏览器，让我们看看他支持哪些浏览器吧：12345678910111213PACKAGE CONTENTS android (package) blackberry (package) chrome (package) common (package) edge (package) firefox (package) ie (package) opera (package) phantomjs (package) remote (package) safari (package) support (package) 可以看到，Selenium支持的浏览器还是非常丰富的，无论是移动端的Android、blackberry还是我们电脑常用的Chrome，Safari，Firefox。然而，在写爬虫的 时候，我推荐使用PhantomJS： PhantomJS是一个而基于webkit的服务器端的JS API。 他全面支持各种原生的Web标准：DOM处理、CSS选择器、JSON、Canvas和SVG。最重要的是他是一个没有GUI的程序， 也就意味着他可以省去大量的加载图形界面的时间。 有人曾经测试过，使用Selenium模块调用上述浏览器，PhantomJS的速度是第一名哦~ 第二和第三是chrome和ie。 以上内容可以概括为selenium可以让我们使用一个无界面的浏览器来打开网页，然后获取网页上的信息，就如同我们正常浏览网页一样。不过，上面作者推荐使用PhantomJS，实际使用中发现，最新版的selenium即将去除对PhantomJS的支持，而且chrome也推出了自己的无头（通俗来讲就是无界面）浏览器。因此，在项目中我们使用chrome。实际使用发现，无头chrome的速度好像比PhantomJS的速度还是要快上那么一丢丢的。 重新获取章节内容首先请pip安装selenium。然后我们需要下载ChromeDriver下载最新版就可以了。看代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 引入webdriver模块from selenium import webdriver# 引入时间模块import time# Keys 是用作关键词输入from selenium.webdriver.common.keys import Keys# 导入chrome配置选项from selenium.webdriver.chrome.options import Options# 生成chrome配置选项chrome_options = Options()# 设置chrome浏览器为无头形式chrome_options.add_argument('--headless')chrome_options.add_argument('--disable-gpu')# 设置chrome浏览器日志级别为最低，减少控制台输出信息chrome_options.add_argument('log-level=3')# 【可选配置】设置chrome浏览器代理# chrome_options.add_argument("--proxy-server=http://127.0.0.1:1080")# 指定chromedriver的路径chrome_position = 'F:\\chromedriver\\chromedriver.exe'# 获取章节内容 def get_content(self, link_list): # 开启chrome浏览器 # executable_path为chromedriver的路径 # chrome_options为chrome配置选项 driver = webdriver.Chrome(executable_path=chrome_position,chrome_options=chrome_options) # 起始页 page = 0 # 总页数 total = len(link_list) # 遍历章节链接列表 for link in link_list: page += 1 # 打开章节页面 driver.get(link) # 等待一段时间，让网页加载完成 driver.implicitly_wait(1) 循环向下拖动页面滚动条40次 for i in range(40): # 拖动滚动条到页面底部 driver.execute_script("window.scrollTo(0,document.body.scrollHeight)") # 每次拖动结束后等待0.02秒，根据自己的网速情况自行设置 time.sleep(0.02) # 获取到章节内容 elem = driver.find_element_by_id("content").text # 调用保存函数将内容保存 self.save_content(elem, self.name) # print(elem.text) # 提升当前章节已下载完成 print('第' + str(page) + '章下载完毕/共' + str(total) + '章') # 提示图书下载完成 print('图书下载完成！') 关闭无头浏览器 driver.quit() 完整程序如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import jsonimport requestsfrom bs4 import BeautifulSoupfrom selenium import webdriverimport time# Keys 是用作关键词输入from selenium.webdriver.common.keys import Keysfrom selenium.webdriver.chrome.options import Optionschrome_options = Options()chrome_options.add_argument('--headless')chrome_options.add_argument('--disable-gpu')chrome_options.add_argument('log-level=3')# chrome_options.add_argument("--proxy-server=http://127.0.0.1:1080")chrome_position = 'F:\\chromedriver\\chromedriver.exe'class GetBook(object): def __init__(self,name): self.name = name self.books = self.get_book_list() self.result = self.get_book_link(self.books,name) if self.result == None: return link_list = self.get_chapter_list(self.result) self.get_content(link_list) # 获取书名列表 def get_book_list(self): with open('book_list.json','r',encoding='utf-8') as f: t = json.load(f) return t # 获取被查询的图书链接 def get_book_link(self,books,name): for book in books: if name == book['name']: print(book['link']) return book['link'] print('你所查询的书名不存在！') return None # 获取章节列表 def get_chapter_list(self, book_link): baseurl = 'http://www.99lib.net' proxie = &#123; 'http': 'http://127.0.0.1:1080', &#125; print('获取章节列表中...') res = requests.get(book_link,proxies=proxie) html = BeautifulSoup(res.text, 'html.parser') a_list = html.select('#dir')[0].find_all('a') link_list = list(map(lambda x: baseurl + x['href'],a_list)) print('本书共有' + str(len(link_list)) + '章') return link_list # 获取章节内容 def get_content(self, link_list): driver = webdriver.Chrome(executable_path=chrome_position,chrome_options=chrome_options) page = 0 total = len(link_list) for link in link_list: page += 1 driver.get(link) # 等待一段时间 driver.implicitly_wait(1) for i in range(40): driver.execute_script("window.scrollTo(0,document.body.scrollHeight)") time.sleep(0.02) # 找到name为"q"的元素 elem = driver.find_element_by_id("content").text self.save_content(elem, self.name) # print(elem.text) print('第' + str(page) + '章下载完毕/共' + str(total) + '章') print('图书下载完成！') driver.quit() def save_content(self, content, name): with open(name + '.txt', 'a', encoding="utf-8") as f: f.write(content + '\n')if __name__ == '__main__': book_name = input('请输入书名:') GetBook(book_name) 本文所提供内容仅供学习测试用途，请勿用作其他用途。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
        <tag>99藏书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫之99藏书网（上）]]></title>
    <url>%2F2018%2F05%2F29%2FPython%E7%88%AC%E8%99%AB%E4%B9%8B99%E8%97%8F%E4%B9%A6%E7%BD%91%EF%BC%88%E4%B8%8A%EF%BC%89%2F</url>
    <content type="text"><![CDATA[99藏书网是一个免费的在线图书阅读网站，网站中分享的均是已出版的书籍，内容还是很多的，而且，校对和排版都非常好。但是，网站不提供下载，而且，右键的复制粘贴也被禁用了。个人比较喜欢看txt电子书，so，咱们写个爬虫来把自己想看的书爬下来吧。 目标分析 图书列表网站图书分类下会显示所有的图书列表，分析链接后发现所有的图书链接均为http://www.99lib.net/book/8947/index.htm这样的格式，其中8947为图书编号，从1-8947。仔细查看列表，每一项中包括，图书的书名，作者，分类，标签以及书籍的简介。为了方便以后功能添加，我们提取书籍的书名，作者，书籍的目录链接以及分类信息，并保存中一个文件中。我们使用chrome开发者工具，定位发现书籍列表为id为list_box的ul列表中。其中，每一项li的中的h2标签中的a标签中包含书籍的目录链接和书名信息。在class为author的h4标签中包含书籍的作者信息，它是下一个h4标签中包含书籍分类信息。 章节列表我们使用chrome开发者工具，定位发现书籍的各章节链接为id为dir的dl列表中dd标签下的a链接中 章节内容任意打开一个章节，从chrom开发者工具中，我们发现章节的所有内容都存放在id为content的div标签中。 项目实施 首先我们把所有的图书信息保存为一个json文件。这里我们使用requests和beautifulsoup库来实现。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import requestsfrom bs4 import BeautifulSoupimport json# 网站的根地址base_url = 'http://www.99lib.net'# 书目页的url模版url = base_url + '/book/index.php?page=&#123;page&#125;'# 书籍的总页数total_page = 1page = 0# 由于网站服务器中美国，这里我设置了代理proxie = &#123; 'http': 'http://127.0.0.1:1080',&#125;# 以追加的方式序列化存储为json并写入文件with open('book_list.json','a',encoding='utf-8') as f: f.write('[\n') # 获取所有书目信息 while True: page += 1 # 如果页面超出总页数，则停止循环 if page &gt; total_page: break # 开始爬取链接 print('fecth:' + url.format(page=page)) res = requests.get(url.format(page=page),proxies=proxie) # 解析爬取到的html html = BeautifulSoup(res.text, 'html.parser') # 获取到总页数 total_page = int(html.select('.total')[0].text[-3:]) # 提取每页书名列表 name_list = html.select('#list_box h2 a') # 提取每页分类列表 categories = html.select('.author') lenth = len(name_list) # 一个简单的循环来生成书籍信息 for j in range(0,lenth): # 一个空字典，每个字典代表一本图书 t = &#123;&#125; # 在字典中添加书籍的各项信息 t['name'] = name_list[j].string t['link'] = base_url + name_list[j]['href'] t['aurhor'] = categories[j].a.string if categories[j].a != None else 'none' t['category'] = categories[j].next_sibling.a.string # 这里需要注意：不提供ensure_ascii=False参数将导致最终生成的文件中unicode编码的文字以\u4e2d\u6587这样的形式显示 if j != lenth - 1: f.write(json.dumps(t,ensure_ascii=False) + ',\n') else: f.write(json.dumps(t,ensure_ascii=False) + '\n') f.write(']\n') 程序执行时的效果如图。程序执行后会在当前目录下生成book_list.json文件，文件打开后内容如图所示咱们下期再见！本文所提供内容仅供学习测试用途，请勿用作其他用途。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
        <tag>99藏书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记--基本的输入输出和循环]]></title>
    <url>%2F2018%2F05%2F23%2FPython%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9F%BA%E6%9C%AC%E7%9A%84%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E5%92%8C%E5%BE%AA%E7%8E%AF%2F</url>
    <content type="text"><![CDATA[输入输出输入input()在Python中，函数input()让程序暂停运行，等待用户输入一些信息，获取用户输入后，将其保存中一个变量中。注意： 函数input接受一个参数，参数内容为要向用户显示的提示或说明。通常为字符串。使用函数input()时，Python将用户的输入解读为字符串，若要解释为其他类型时，需要使用指定的转换函数。 int() 转换为数值类型% 求模运算符 将两个数相除并返回余数 输出print()在Python中，函数print()让程序在控制台中打印一段内容。通常为字符串。如果为非字符串的变量，则会调用其toString方法。 循环while循环while循环不断运行，直到指定的条件不满足为止。 while循环通常会指定一个特殊的标志，当标志满足某个条件时，停止循环的运行除了指定退出循环的标志，还可以用关键字break来退出循环continue，continue关键字不会像break那样直接退出整个循环，它会结束当前循环，并开始下一个循环。每个while循环都必须有停止运行的途径，以免出现死循环。可以使用while循环来循环列表或者根据用户输入生产字典。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>输入输出</tag>
        <tag>循环</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记--字典]]></title>
    <url>%2F2018%2F05%2F17%2FPython%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%AD%97%E5%85%B8%2F</url>
    <content type="text"><![CDATA[字典在Python中，字典是一系列键-值对。类似于JS中的对象。 增删改查增加dict[‘key’] = value 删除del dict[‘key’] 修改dict[‘key’] = new_value 查询dict[‘key’] 遍历字典遍历字典中的键值对12for key, value in dict.items(): do something with key and value 方法items()返回一个键-值对的列表 仅遍历字典中的键12for key in dict.keys(): do something with key 方法keys返回一个键的列表 按顺序遍历字典中所有的键12for key in sorted(dict.keys()): do something with key 对keys()方法返回的列表进行排序 遍历字典中所有的值12for key in dict.values(): do something with key 方法values返回一个值的列表 字典与列表的嵌套有时候，为了复杂的需求，需要将一系列字典存储中列表中，或者将列表作为值存储中字典中，称为嵌套。同时，字典也可以嵌套在字典中，列表也可以嵌套中列表中。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>学习</tag>
        <tag>字典</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记--列表]]></title>
    <url>%2F2018%2F05%2F17%2FPython%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--%E5%88%97%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[列表list 对应于js中的数组，由一系列按特定顺序排列的元素组成，通过索引来访问指定位置的元素。 增删改查增加 list.append(‘jack’) 在末尾添加 list.insert(2，’rose’) 在指定索引处插入 删除 del list[1] 删除指定索引处的元素 list.pop() 删除列表末尾的元素并返回该元素 list.pop(index) 删除列表指定索引出的元素并返回该元素 list.remove(value) 从列表中删除指定值的元素并返回该元素 修改通过给指定索引的元素赋值可以进行修改 查询通过索引查询指定索引处的元素 组织列表排序 list.sort() 对列表进行永久排序，可选参数reverse = True,表示按照字母从大到小排序 list.sorted() 用法同sort(),不修改原列表 反转list.reverse() 将列表反转，改变原列表 长度len(list) 获取列表的长度 操作列表12for item in list: print(item) 创建数值列表range(a,b,c) 左闭右开，包括a,不包括b,c为步长list(range(a,b)) 生成数值列表 数值列表的专用函数 min() 最小值 max() 最大值 sum() 求和 列表表达式12[m + n for m in 'ABC' for n in 'XYZ'][x * x for x in range(1,11)] 切片截取列表部分元素组成的子列表list[a:b:c] 从列表索引为a的元素开始取到索引为b的元素（不包括b），步长为ca省略则默认从索引为0开始，b省略则默认截取到列表最后一位c省略则默认步长为1 list[:10] 前十个list[-10:] 后十个list[10:20]11到20list[:] 复制列表 元组元组是不可更改值的列表，要更改元组的值，只能给变量重新赋值。特殊的 当元组的值为引用类型的数据时，可以更改引用类型数据内的值，但其实，元组的值并未改变，仍指向指定的内存空间 条件判断 and 逻辑与，两个条件均满足，结果为True，否则为False or 逻辑或，两个条件均不满足，结果为False，否则为True not 逻辑非，取反 in 是否包含，包含为True，否则为False not in 是否不包含，包含为False，否则为True ==, &gt;=, &lt;=, !=]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>笔记</tag>
        <tag>列表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用hexo搭建自己的个人博客]]></title>
    <url>%2F2018%2F05%2F02%2F%E7%94%A8hexo%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[宝剑锋从磨砺出，梅花香自苦寒来。 《警世贤文·勤奋篇》 引言从开始从事前端开发，一直就想搭建一个属于自己的博客。但由于各种原因，最后都不了了之。最近有了点空闲时间，于是参照着网上的教程，搭建了自己的博客。过程中也遇到了一些问题，有些问题自己还没找到答案。现在把自己在搭建过程中的一些经验和问题总结如下： 环境准备 安装Hexo 初探Hexo 部署Hexo 发表文章 使用Next主题 小结 1.环境准备1.1 注册github账号由于咱们的博客最后要发布到github上，所以你首先得有一个github帐号注册地址 1.2 安装Git Bash由于我用的windows，所以安装的是windows版本，下载地址如下：下载地址安装没什么好讲的，一路next就ok安装好之后，可以通过鼠标右键-&gt;Git Bash Here 打开gitbash,如图：打开窗口后，你可以通过git version命令来查看当前安装的版本，如图可以看到，我安装的是2.16.2版本 Git是目前世界上最先进的分布式版本控制系统（没有之一）。如果你对于git不了解，请前去廖雪峰大大的博客学习他的git教程 1.3 安装NodeJs下载地址(LTS为长期支持版，或者说稳定版，Current为最新版)Hexo是基于nodeJs环境的静态博客，依赖于npm工具，请务必安装NodeJs。建议安装LTS版本安装过程仍然时一路next，唯一需要注意的是：在Custom Setup这一步记得选 Add to PATH ,这样你就不用自己去配置电脑上环境变量了。然后在cmd命令窗口下输入node -v命令，如果能看到类似与下图的内容，证明你安装正确，可以进行下一步了至此，我们的环境准备就完成了。现在，让我们开始Hexo吧！ 2.安装HexoHexo的安装很简单，我们先创建一个文件夹blog，来存放所有我们博客的东西。创建好文件夹后，我们打开这个文件夹并在空白处按住shift单击鼠标右键，然后我们能看到下图红框所示单击在此处打开命令窗口，我们进入cmd命令行然后我们用npm全局安装 Hexo，命令如下：1npm i hexo -g 安装完成后，可以通过hexo -v命令来查看版本 接下来，我们初始化自己的hexo博客文件，命令hexo init一切顺利的话，在blog文件夹下，可以看到如下文件 简单注释一下： node_modules npm—下载的相关依赖包，public————-存放最后生成的我们博客的静态文件scaffolds———-生成md文档的模板文件scripts————这个文件夹不是自动生成的，是我个人写的一个脚本文件source————-这个文件夹中的文件是生成public中文件的源文件themes————-主题文件夹.gitignore———上传到git时需要忽略的文件_config.yml——–博客的站点配置文件db.json————自动生成的db文件package.json——-npm的包管理文件 其他未注释的文件不用管 3.初探Hexo现在我们已经完成了Hexo的安装，先来小小的使用一下，在cmd命令行中依次执行以下命令：123hexo clean //清理public文件夹hexo g //在public文件夹生成静态文件hexo s //开启本地服务器 在你的浏览器中输入localhost:4000，你就应该能看到自己的博客了如果你看不到这个页面，可能是你的4000端口已经被占用了，将上面最后一行命令修改为：1hexo s -p 4001 4.部署Hexo现在我们已经让我们的博客在自己的本地运行起来了，但是，博客是用来分享的，只有自己能看到是不行的。因此，我们要把它发布到网络上，让其他人能够看到。这里我们选择发布到github的gh-pages。原因当然是因为这是 免费的。 4.1 创建repo并开启gh-pages登录你之前注册的github账户，按图操作 接下来打开你的gitbash（见上文1.2），配置github信息（yourname和youremail都替换成你自己的） 接下来在gitbash中输入（youremail@example.com为你的email地址）1ssh-keygen -t rsa -C &quot;youremail@example.com&quot; 连续回车，直到生成自己的ssh信息，然后在C:\Users\你的用户名\.ssh文件夹中找到id_rsa.pub文件，在记事本中打开，复制所有文件内容然后登录github 4.2 部署博客到gh-pages用编辑器打开blog目录下的_config.yml文件，修改配置如下1234deploy: type: git repo: https://github.com/yourgithubname/yourgithubname.github.io.git branch: master yourgithubname为你的github名字每项配置冒号后面有一个半角空格，不可省略 保存退出后，在blog目录下，打开cmd命令行窗口，执行1npm install hexo-deployer-git --save 现在执行最后一步（以后每次更新博客时，都建议按照如下步骤操作）123hexo cleanhexo ghexo d 最后一步执行过程中可能需要输入你的github用户名和密码，按提示操作当年看到下图，说明你的博客已经部署成功 在浏览器中打开http://yourgithubname.github.io，就可以看到你的个人博客了。 5.发表文章我们的个人博客已经部署完成，怎么更新文章呢？有两种方法 在blog文件夹的_posts文件夹下，新建.md文档 在blog文件夹中打开cmd，执行hexo new 文章标题 文章写好后，执行：123hexo cleanhexo ghexo d 你的文章就被更新到博客上来，是不是很简单？ 6.使用Next主题Hexo默认使用的时自带的landscape主题，但是hexo有大量的第三方提供的主题可供选择，并且比自带的主题美观的多。在这里，向大家推荐github上stars数最多的Next主体，美观大方，支持几种不同的风格。而且官方的中文说明文档很详细，对我们这种新手来说比较友好。Next主题的安装非常简单，进入你的博客根目录，打开gitbash，执行1git clone https://github.com/theme-next/hexo-theme-next themes/next 在你的themes文件夹下会多出来一个next文件夹然后打开你博客根目录下的_config.yml文件，找到theme字段，将后面的landscape改为next(冒号后面的半角空格一定要有)，保存即可 重新发布你的博客，看看它现在的样子吧！ 7.小结本篇文章中，我们使用Hexo搭建了一个属于自己的博客，完成了对于Hexo默认主题的更换，并将我们的博客部署到了github上。下一篇文章，我们将要对自己的博客进行一些具体的设置，并添加一些新的功能。本文在完成过程中参考了很多作者使用Hexo搭建博客的文章，非常感谢他们的分享让我在这一过程中少走了很多弯路。在此一并感谢！hexo从零开始到搭建完整Hexo博客搭建及NexT主题个性化设置手把手教你用Hexo+Github 搭建属于自己的博客用Hexo + github搭建自己的博客 — 再也不用羡慕别人了！]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一篇博文]]></title>
    <url>%2F2018%2F04%2F28%2F%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E6%96%87%2F</url>
    <content type="text"><![CDATA[人的一切痛苦，本质上都是对自己的无能的愤怒 王小波一直想要开一个自己的博客，但又觉得自己才疏学浅，难堪大用，写博客这种事完全就是误人子弟，所以就不了了之了嗯，为自己的懒癌找了个完美的借口。最近新换了工作，新的开始。嗯，最重要是我的懒癌最近懒的找我麻烦。所以，给自己搭了个小博客，希望自己能够坚持住。嗯，加油！我是最胖棒的！ 一起来写博客吧！]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
</search>
